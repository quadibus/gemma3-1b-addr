{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bdd37b96-43ec-4713-9336-9136ce7b999e",
   "metadata": {},
   "source": [
    "# installation dependances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a0d1d20-6537-4ff7-bfa0-7efbada78d77",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "import os\n",
    "\n",
    "print(os.environ.keys())\n",
    "\n",
    "if \"COLAB_\" not in \"\".join(os.environ.keys()):\n",
    "    !pip install unsloth vllm\n",
    "else:\n",
    "    # [NOTE] Do the below ONLY in Colab! Use [[pip install unsloth vllm]]\n",
    "    !pip install --no-deps unsloth vllm\n",
    "# Install latest Hugging Face for Gemma-3!\n",
    "!pip install --no-deps git+https://github.com/huggingface/transformers@v4.49.0-Gemma-3\n",
    "!pip install -U ipywidgets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6475ef86-e31c-45e2-b9c2-5292e7d38754",
   "metadata": {},
   "source": [
    "# configuration du model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03f3f7b0-0186-4ec7-a073-a0be0d006415",
   "metadata": {},
   "outputs": [],
   "source": [
    "from unsloth import FastModel\n",
    "import torch\n",
    "\n",
    "max_seq_length = 2048\n",
    "\n",
    "model, tokenizer = FastModel.from_pretrained(\n",
    "    #model_name = \"unsloth/gemma-3-1b-pt-unsloth-bnb-4bit\",\n",
    "    model_name = \"google/gemma-3-1b-pt\",\n",
    "    max_seq_length = max_seq_length, # Choose any for long context!\n",
    "    load_in_4bit = True,  # 4 bit quantization to reduce memory\n",
    "    load_in_8bit = False, # [NEW!] A bit more accurate, uses 2x memory\n",
    "    full_finetuning = False, # [NEW!] We have full finetuning now!\n",
    "    # token = \"hf_...\", # use one if using gated models\n",
    ")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47d72bbb-5034-450e-a1d4-3d92ae830aa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model = FastModel.get_peft_model(\n",
    "    model,\n",
    "    finetune_vision_layers     = False, # Turn off for just text!\n",
    "    finetune_language_layers   = True,  # Should leave on!\n",
    "    finetune_attention_modules = False,  # Attention good for GRPO\n",
    "    finetune_mlp_modules       = True,  # SHould leave on always!\n",
    "\n",
    "    r = 8,           # Larger = higher accuracy, but might overfit\n",
    "    lora_alpha = 8,  # Recommended alpha == r at least\n",
    "    lora_dropout = 0,\n",
    "    bias = \"none\",\n",
    "    random_state = 3407,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e902e7aa-825d-4e56-9a53-7bf8dcb404fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "# Load CSV into Hugging Face dataset\n",
    "dataset = load_dataset(\"csv\", data_files=\"Jeu_SLM_enrichi_avec_rues_compos_es__tirets_.csv\")\n",
    "\n",
    "# Access the split (train by default)\n",
    "train_dataset = dataset[\"train\"]\n",
    "\n",
    "# Preview\n",
    "print(train_dataset[0])\n",
    "train_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25dad3f2-8bc8-491e-9014-a1da0f6e3c4d",
   "metadata": {},
   "source": [
    "# configuration de l'entrainement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54df6f3f-737d-491b-afe7-dd62823a543f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "def format_example(example):\n",
    "    return {\n",
    "        \"text\": f\"{example['prompt']}\"+tokenizer.eos_token\n",
    "    }\n",
    "\n",
    "formatted_dataset = train_dataset.map(format_example)\n",
    "formatted_dataset = formatted_dataset.remove_columns(\n",
    "    [col for col in formatted_dataset.column_names if col != \"text\"]\n",
    ")\n",
    "\n",
    "print(formatted_dataset[0])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d425280-2775-422b-84aa-961b91574e44",
   "metadata": {},
   "outputs": [],
   "source": [
    "from trl import SFTTrainer, SFTConfig\n",
    "\n",
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    train_dataset=formatted_dataset,\n",
    "    args=SFTConfig(\n",
    "        max_seq_length=2048,\n",
    "        per_device_train_batch_size=2,\n",
    "        gradient_accumulation_steps=4,\n",
    "        warmup_steps=5,\n",
    "        max_steps=100,\n",
    "        learning_rate=2e-4,\n",
    "        logging_steps=1,\n",
    "        optim=\"adamw_8bit\",\n",
    "        weight_decay=0.01,\n",
    "        lr_scheduler_type=\"linear\",\n",
    "        seed=3407,\n",
    "        report_to=\"none\",\n",
    "    ),\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3a1b020-dfa6-46da-af09-9fa5b835244b",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.decode(trainer.train_dataset[100][\"input_ids\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5317d1ce-c576-4e98-bff9-5c3a5a26c162",
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title Show current memory stats\n",
    "gpu_stats = torch.cuda.get_device_properties(0)\n",
    "start_gpu_memory = round(torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3)\n",
    "max_memory = round(gpu_stats.total_memory / 1024 / 1024 / 1024, 3)\n",
    "print(f\"GPU = {gpu_stats.name}. Max memory = {max_memory} GB.\")\n",
    "print(f\"{start_gpu_memory} GB of memory reserved.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7e1a4d7-c0f0-45dd-bdf7-c6446891ccfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer_stats = trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfca4506-309c-47ac-b35c-52d3fdc50a65",
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title Show final memory and time stats\n",
    "used_memory = round(torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3)\n",
    "used_memory_for_lora = round(used_memory - start_gpu_memory, 3)\n",
    "used_percentage = round(used_memory / max_memory * 100, 3)\n",
    "lora_percentage = round(used_memory_for_lora / max_memory * 100, 3)\n",
    "print(f\"{trainer_stats.metrics['train_runtime']} seconds used for training.\")\n",
    "print(\n",
    "    f\"{round(trainer_stats.metrics['train_runtime']/60, 2)} minutes used for training.\"\n",
    ")\n",
    "print(f\"Peak reserved memory = {used_memory} GB.\")\n",
    "print(f\"Peak reserved memory for training = {used_memory_for_lora} GB.\")\n",
    "print(f\"Peak reserved memory % of max memory = {used_percentage} %.\")\n",
    "print(f\"Peak reserved memory for training % of max memory = {lora_percentage} %.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76c7c5ed-0593-49b4-b0dd-e94a2fe77714",
   "metadata": {},
   "source": [
    "# Save It"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7b7915c-c8ab-436e-b6f4-5bb29cca20bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "raise SystemExit(\"Execution stopped here on purpose.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ad5875e-bbe3-4f0b-89a4-78b6ecac688a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#trainer.save_model(\"gemma3-address-parser\")\n",
    "model.save_pretrained(\"gemma3-address-parser-lora\", save_adapter=True)\n",
    "tokenizer.save_pretrained(\"gemma3-address-parser-lora\")\n",
    "\n",
    "model.config.save_pretrained(\"gemma3-address-parser-base\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bccd8b4f-aca0-44c4-80f4-820dfdfd48c0",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from unsloth import FastModel\n",
    "model, tokenizer = FastModel.from_pretrained(\n",
    "    model_name = \"gemma3-address-parser-lora\", # YOUR MODEL YOU USED FOR TRAINING\n",
    "    max_seq_length = 2048,\n",
    "    load_in_4bit = True,\n",
    ")\n",
    "\n",
    "address = \"Leclerc 10 bis route Victor Hugo 92200 Neuilly-sur-Seine\"\n",
    "address = \"Nathalie Dubois, 25 Rue du Faubourg Saint-Antoine, 06000 Nice, France\"\n",
    "\n",
    "prompt = f\"Parsing: {address} \\nChamps:\"\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\").to(\"cuda\")\n",
    "\n",
    "outputs = model.generate(\n",
    "    **inputs,\n",
    "    max_new_tokens=100,\n",
    "    temperature = 1.0, top_p = 0.95, top_k = 64,\n",
    "    do_sample=False,\n",
    "    eos_token_id=tokenizer.eos_token_id,\n",
    "    pad_token_id=tokenizer.pad_token_id\n",
    ")\n",
    "\n",
    "print(tokenizer.decode(outputs[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1a73ab2-5abb-44c6-97bc-689811064a6a",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Change to True to save to GGUF\n",
    "model.save_pretrained_merged(\"gemma3-address-parser-finetune\", tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "291a533f-ead3-4597-85c8-394bd2d0957b",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "model.save_pretrained_gguf(\"gemma3-address-parser-finetune\",\n",
    "    quantization_type = \"F16\", # For now only Q8_0, BF16, F16 supported\n",
    ")\n",
    "model.save_pretrained_gguf(\"gemma3-address-parser-finetune\",\n",
    "    quantization_type = \"Q8_0\", # For now only Q8_0, BF16, F16 supported\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07c00f65-f010-4df7-8ad6-fa9a7aaf5606",
   "metadata": {},
   "source": [
    "# run it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3ab4059-ecca-4c80-83ed-03afd1851ab1",
   "metadata": {},
   "outputs": [],
   "source": [
    "address = \"Leclerc 10 bis route Victor Hugo 92200 Neuilly-sur-Seine\"\n",
    "\n",
    "prompt = f\"Parsing: {address} \\nChamps:\"\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\").to(\"cuda\")\n",
    "\n",
    "outputs = model.generate(\n",
    "    **inputs,\n",
    "    max_new_tokens=100,\n",
    "    temperature = 1.0, top_p = 0.95, top_k = 64,\n",
    "    do_sample=False,\n",
    "    eos_token_id=tokenizer.eos_token_id,\n",
    "    pad_token_id=tokenizer.pad_token_id\n",
    ")\n",
    "\n",
    "print(tokenizer.decode(outputs[0], skip_special_tokens=True))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a685ddf-afdf-4fe8-b56f-b36463857aa1",
   "metadata": {},
   "source": [
    "# use llama to run gguf model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cbb5901-fb6e-42c6-98e1-02fe81d76870",
   "metadata": {},
   "outputs": [],
   "source": [
    "raise SystemExit(\"Execution stopped here on purpose.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e766be9a-2b2a-48b4-98d8-6391597f331f",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install llama-cpp-python\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2fdc882-b826-4ee7-90e9-995694367b29",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from llama_cpp import Llama\n",
    "\n",
    "import logging\n",
    "#logging.getLogger(\"llama_cpp\").setLevel(logging.WARNING)\n",
    "\n",
    "llm = Llama(\n",
    "    model_path=\"gemma3-address-parser-finetune.F16.gguf\",  # your gguf file path\n",
    "    n_ctx=32768,\n",
    "    n_threads=72,\n",
    "    verbose=False# adjust to your CPU\n",
    ")\n",
    "#address = \"Leclerc 10 bis route Victor Hugo 92200 Neuilly-sur-Seine\"\n",
    "address = \"Nathalie Dubois, 25 Rue du Faubourg Saint-Antoine, 06000 Nice, France\"\n",
    "prompt = f\"Parsing: {address} \\nChamps:\"\n",
    "\n",
    "response = llm(prompt, max_tokens=2048)\n",
    "print(response[\"choices\"][0][\"text\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00a12ed3-c0db-4fcd-8ef8-7a3471471152",
   "metadata": {},
   "source": [
    "# against test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c24a2a5e-745e-4378-a36a-33fb64ea02b5",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import math\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "def is_equivalent(expected, found):\n",
    "    if pd.isna(expected) and (found is None or (isinstance(found, float) and math.isnan(found))):\n",
    "        return True\n",
    "    return expected == found\n",
    "\n",
    "# Load CSV file\n",
    "df = pd.read_csv(\"fr-test-dataset2.csv\") \n",
    "expected_fields = ['Building_Number' \n",
    "                   , 'City', 'Country', 'Recipient', 'Street_Name', 'Zip_Code', 'repetition', 'type_voie']\n",
    "\n",
    "\n",
    "# path to your CSV\n",
    "from unsloth import FastModel\n",
    "model, tokenizer = FastModel.from_pretrained(\n",
    "    model_name = \"gemma3-address-parser-lora\", # YOUR MODEL YOU USED FOR TRAINING\n",
    "    max_seq_length = 2048,\n",
    "    load_in_4bit = True,\n",
    ")\n",
    "\n",
    "\n",
    "model.eval()\n",
    "\n",
    "exact_match = 0\n",
    "field_total = 0\n",
    "field_correct = 0\n",
    "address_parsed = 0\n",
    "sorties = []\n",
    "for _, row in tqdm(df.iterrows(), total=len(df)):\n",
    "    address_parsed+=1\n",
    "    if address_parsed ==10: break\n",
    "        \n",
    "    prompt = f\"Parsing: {row['Address']} \\nChamps:\"\n",
    "\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(\"cuda\")\n",
    "    outputs = model.generate(\n",
    "        **inputs,\n",
    "        max_new_tokens=100,\n",
    "        temperature = 1.0, top_p = 0.95, top_k = 64,\n",
    "        do_sample=False,\n",
    "        eos_token_id=tokenizer.eos_token_id,\n",
    "        pad_token_id=tokenizer.pad_token_id\n",
    "    )\n",
    "    \n",
    "    pred = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    pred_json_only = pred.split(\"Champs:\")[1].strip()\n",
    "    #print(\"pred :: \"+pred)\n",
    "    #print(\"pred json only :: \"+ pred_json_only)\n",
    "    try:\n",
    "        pred_json = json.loads(pred_json_only)\n",
    "\n",
    "    except:\n",
    "        pred_json = {}\n",
    "\n",
    "    match = True\n",
    "    #print(\"pred json :: \"+ str(pred_json))\n",
    "    for field in expected_fields:\n",
    "        expected_value = row[field]\n",
    "        #print(\"field name::\" + field)\n",
    "        #print(\"exepcted value ::\"+expected_value)\n",
    "        field_total += 1\n",
    "        found_value = pred_json.get(field)\n",
    "        if isinstance(found_value, str) and isinstance(expected_value, str):\n",
    "            found_value = found_value.lower()\n",
    "            expected_value = expected_value.lower()\n",
    "        #print(\"found value ::\"+ str(pred_json.get(field)) )\n",
    "        if is_equivalent(row[field], pred_json.get(field)):\n",
    "            field_correct += 1\n",
    "        else: \n",
    "            if str(found_value) == str(expected_value):\n",
    "                field_correct += 1\n",
    "            else:\n",
    "                match = False\n",
    "                print(f\"adresse : {str(row['Address'])}\")\n",
    "                print(f\"field : {str(field)} :: expected : {str(expected_value)} :: found : {str(found_value)} \")\n",
    "\n",
    "    if match:\n",
    "        exact_match += 1\n",
    "        print(f\"ok : {str(row['Address'])}\")\n",
    "\n",
    "    sorties.append({\n",
    "        \"demande\": row['Address'],\n",
    "        \"prediction\": pred_json,\n",
    "        \"attendue\": expected_fields,\n",
    "        \"match\": match\n",
    "    })\n",
    "    \n",
    "\n",
    "# Results\n",
    "total = len(df)\n",
    "total = 10\n",
    "print(f\"\\nExact Match Accuracy: {exact_match / total:.2%}\")\n",
    "print(f\"\\nExact Match : {str(exact_match)}\")\n",
    "print(f\"Field-Level Accuracy: {field_correct / field_total:.2%}\")\n",
    "print(f\"Field-Level : {str(field_correct)}\")\n",
    "print(f\"total field : {str(3)}\")"
   ]
  },
  {
   "cell_type": "raw",
   "id": "7ac96129-32a1-496e-817d-b936cdea9b4f",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23965f96-100a-400c-86d5-bbceb6a8d1f7",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
