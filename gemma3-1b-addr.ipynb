{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bdd37b96-43ec-4713-9336-9136ce7b999e",
   "metadata": {},
   "source": [
    "# installation dependances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3a0d1d20-6537-4ff7-bfa0-7efbada78d77",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "import os\n",
    "\n",
    "print(os.environ.keys())\n",
    "\n",
    "if \"COLAB_\" not in \"\".join(os.environ.keys()):\n",
    "    !pip install unsloth vllm\n",
    "else:\n",
    "    # [NOTE] Do the below ONLY in Colab! Use [[pip install unsloth vllm]]\n",
    "    !pip install --no-deps unsloth vllm\n",
    "# Install latest Hugging Face for Gemma-3!\n",
    "!pip install --no-deps git+https://github.com/huggingface/transformers@v4.49.0-Gemma-3\n",
    "!pip install -U ipywidgets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6475ef86-e31c-45e2-b9c2-5292e7d38754",
   "metadata": {},
   "source": [
    "# configuration du model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "03f3f7b0-0186-4ec7-a073-a0be0d006415",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ü¶• Unsloth: Will patch your computer to enable 2x faster free finetuning.\n",
      "ü¶• Unsloth Zoo will now patch everything to make training faster!\n",
      "INFO 04-08 11:37:59 [__init__.py:239] Automatically detected platform cuda.\n",
      "==((====))==  Unsloth 2025.3.19: Fast Gemma3 patching. Transformers: 4.50.0.dev0. vLLM: 0.8.3.\n",
      "   \\\\   /|    Tesla T4. Num GPUs = 1. Max memory: 14.568 GB. Platform: Linux.\n",
      "O^O/ \\_/ \\    Torch: 2.6.0+cu124. CUDA: 7.5. CUDA Toolkit: 12.4. Triton: 3.2.0\n",
      "\\        /    Bfloat16 = FALSE. FA [Xformers = 0.0.29.post2. FA2 = False]\n",
      " \"-____-\"     Free license: http://github.com/unslothai/unsloth\n",
      "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n",
      "Unsloth: Using float16 precision for gemma3 won't work! Using float32.\n"
     ]
    }
   ],
   "source": [
    "from unsloth import FastModel\n",
    "import torch\n",
    "\n",
    "max_seq_length = 2048\n",
    "\n",
    "model, tokenizer = FastModel.from_pretrained(\n",
    "    model_name = \"unsloth/gemma-3-1b-pt-unsloth-bnb-4bit\",\n",
    "    max_seq_length = max_seq_length, # Choose any for long context!\n",
    "    load_in_4bit = True,  # 4 bit quantization to reduce memory\n",
    "    load_in_8bit = False, # [NEW!] A bit more accurate, uses 2x memory\n",
    "    full_finetuning = False, # [NEW!] We have full finetuning now!\n",
    "    # token = \"hf_...\", # use one if using gated models\n",
    ")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "47d72bbb-5034-450e-a1d4-3d92ae830aa6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unsloth: Making `model.base_model.model.model` require gradients\n"
     ]
    }
   ],
   "source": [
    "\n",
    "model = FastModel.get_peft_model(\n",
    "    model,\n",
    "    finetune_vision_layers     = False, # Turn off for just text!\n",
    "    finetune_language_layers   = True,  # Should leave on!\n",
    "    finetune_attention_modules = False,  # Attention good for GRPO\n",
    "    finetune_mlp_modules       = True,  # SHould leave on always!\n",
    "\n",
    "    r = 8,           # Larger = higher accuracy, but might overfit\n",
    "    lora_alpha = 8,  # Recommended alpha == r at least\n",
    "    lora_dropout = 0,\n",
    "    bias = \"none\",\n",
    "    random_state = 3407,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e902e7aa-825d-4e56-9a53-7bf8dcb404fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Address': 'Supermarch√© Carrefour, 147 Rue de Rivoli, 75001 Paris, France', 'Building_Number': 147, 'City': 'Paris', 'Recipient': 'Supermarch√© Carrefour', 'Street_Name_old': 'Rue de Rivoli', 'Street_Name': 'de Rivoli', 'type_voie': 'Rue', 'Zip_Code': 75001, 'Country': 'France', 'repetition': None}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['Address', 'Building_Number', 'City', 'Recipient', 'Street_Name_old', 'Street_Name', 'type_voie', 'Zip_Code', 'Country', 'repetition'],\n",
       "    num_rows: 2695\n",
       "})"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "# Load CSV into Hugging Face dataset\n",
    "dataset = load_dataset(\"csv\", data_files=\"fr-train-dataset2.csv\")\n",
    "\n",
    "# Access the split (train by default)\n",
    "train_dataset = dataset[\"train\"]\n",
    "\n",
    "# Preview\n",
    "print(train_dataset[0])\n",
    "train_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2d855e76-6dc8-4131-8cd8-6824550eecd4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Supermarch√© Carrefour, 147 Rue de Rivoli, 75001 Paris, France'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset[0][\"Address\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "dc3b2779-19e0-4fef-853b-64bf6e743488",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Address': 'Supermarch√© Carrefour, 147 Rue de Rivoli, 75001 Paris, France', 'Building_Number': 147, 'City': 'Paris', 'Recipient': 'Supermarch√© Carrefour', 'Street_Name_old': 'Rue de Rivoli', 'Street_Name': 'de Rivoli', 'type_voie': 'Rue', 'Zip_Code': 75001, 'Country': 'France', 'repetition': None, 'question': 'Supermarch√© Carrefour, 147 Rue de Rivoli, 75001 Paris, France', 'answer': {'Building_Number': 147, 'City': 'Paris', 'Country': 'France', 'Recipient': 'Supermarch√© Carrefour', 'Street_Name': 'de Rivoli', 'Zip_Code': 75001, 'repetition': None, 'type_voie': 'Rue'}}\n",
      "['Address', 'Building_Number', 'City', 'Recipient', 'Street_Name_old', 'Street_Name', 'type_voie', 'Zip_Code', 'Country', 'repetition', 'question', 'answer']\n"
     ]
    }
   ],
   "source": [
    "# Define columns to exclude from tgt\n",
    "exclude_cols = [\"Address\", \"Street_Name_old\"]  # üëà Add any column to ignore here\n",
    "\n",
    "# Determine tgt columns\n",
    "tgt_cols = [col for col in train_dataset.column_names if col not in exclude_cols]\n",
    "src_col = \"Address\"\n",
    "\n",
    "\n",
    "# Transform\n",
    "def transform(example):\n",
    "    return {\n",
    "        \"question\": example[src_col],\n",
    "        \"answer\": {col: example[col] for col in tgt_cols}\n",
    "    }\n",
    "\n",
    "dataset_refined = train_dataset.map(transform)\n",
    "\n",
    "\n",
    "# Preview\n",
    "print(dataset_refined[0])\n",
    "\n",
    "\n",
    "print(dataset_refined.column_names)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ad49ff07-dec4-42a9-843e-17fc098b4eb4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Building_Number': 147,\n",
       " 'City': 'Paris',\n",
       " 'Country': 'France',\n",
       " 'Recipient': 'Supermarch√© Carrefour',\n",
       " 'Street_Name': 'de Rivoli',\n",
       " 'Zip_Code': 75001,\n",
       " 'repetition': None,\n",
       " 'type_voie': 'Rue'}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_refined[0][\"answer\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5c89a462-74d7-488e-b459-fabf96d4a4ac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Supermarch√© Carrefour, 147 Rue de Rivoli, 75001 Paris, France'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_refined[0][\"question\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25dad3f2-8bc8-491e-9014-a1da0f6e3c4d",
   "metadata": {},
   "source": [
    "# configuration de l'entrainement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "54df6f3f-737d-491b-afe7-dd62823a543f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'text': 'Parsing: Supermarch√© Carrefour, 147 Rue de Rivoli, 75001 Paris, France \\nChamps: {\"Building_Number\": 147, \"City\": \"Paris\", \"Country\": \"France\", \"Recipient\": \"Supermarch√© Carrefour\", \"Street_Name\": \"de Rivoli\", \"Zip_Code\": 75001, \"repetition\": null, \"type_voie\": \"Rue\"}<eos>'}\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "def format_example(example):\n",
    "    return {\n",
    "        \"text\": f\"Parsing: {example['question']} \\nChamps: {json.dumps(example['answer'], ensure_ascii=False)}\"+tokenizer.eos_token\n",
    "    }\n",
    "\n",
    "formatted_dataset = dataset_refined.map(format_example)\n",
    "formatted_dataset = formatted_dataset.remove_columns(\n",
    "    [col for col in formatted_dataset.column_names if col != \"text\"]\n",
    ")\n",
    "\n",
    "print(formatted_dataset[0])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9d425280-2775-422b-84aa-961b91574e44",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unsloth: Switching to float32 training since model cannot work with float16\n"
     ]
    }
   ],
   "source": [
    "from trl import SFTTrainer, SFTConfig\n",
    "\n",
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    train_dataset=formatted_dataset,\n",
    "    args=SFTConfig(\n",
    "        max_seq_length=2048,\n",
    "        per_device_train_batch_size=2,\n",
    "        gradient_accumulation_steps=4,\n",
    "        warmup_steps=5,\n",
    "        max_steps=30,\n",
    "        learning_rate=2e-4,\n",
    "        logging_steps=1,\n",
    "        optim=\"adamw_8bit\",\n",
    "        weight_decay=0.01,\n",
    "        lr_scheduler_type=\"linear\",\n",
    "        seed=3407,\n",
    "        report_to=\"none\",\n",
    "    ),\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c3a1b020-dfa6-46da-af09-9fa5b835244b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<bos>Parsing: 79 Place de la R√©publique, 35000 Rennes, France \\nChamps: {\"Building_Number\": 79, \"City\": \"Rennes\", \"Country\": \"France\", \"Recipient\": null, \"Street_Name\": \"de la R√©publique\", \"Zip_Code\": 35000, \"repetition\": null, \"type_voie\": \"Place\"}<eos>'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(trainer.train_dataset[100][\"input_ids\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5317d1ce-c576-4e98-bff9-5c3a5a26c162",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU = Tesla T4. Max memory = 14.568 GB.\n",
      "1.66 GB of memory reserved.\n"
     ]
    }
   ],
   "source": [
    "# @title Show current memory stats\n",
    "gpu_stats = torch.cuda.get_device_properties(0)\n",
    "start_gpu_memory = round(torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3)\n",
    "max_memory = round(gpu_stats.total_memory / 1024 / 1024 / 1024, 3)\n",
    "print(f\"GPU = {gpu_stats.name}. Max memory = {max_memory} GB.\")\n",
    "print(f\"{start_gpu_memory} GB of memory reserved.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c7e1a4d7-c0f0-45dd-bdf7-c6446891ccfd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "==((====))==  Unsloth - 2x faster free finetuning | Num GPUs used = 1\n",
      "   \\\\   /|    Num examples = 2,695 | Num Epochs = 1 | Total steps = 30\n",
      "O^O/ \\_/ \\    Batch size per device = 2 | Gradient accumulation steps = 4\n",
      "\\        /    Data Parallel GPUs = 1 | Total batch size (2 x 4 x 1) = 8\n",
      " \"-____-\"     Trainable parameters = 5,031,936/1,000,000,000 (0.50% trained)\n",
      "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='30' max='30' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [30/30 00:55, Epoch 0/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>2.739000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>2.711800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>2.670300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>2.718200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>2.671200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>2.533000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>2.325800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>2.200300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>2.031300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>1.873000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>1.850400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>1.644500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>1.520200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>1.488900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>1.287700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>1.215800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>1.158600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>1.141400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>1.038500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>0.985800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21</td>\n",
       "      <td>0.989800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22</td>\n",
       "      <td>0.906600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23</td>\n",
       "      <td>0.843400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24</td>\n",
       "      <td>0.791300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25</td>\n",
       "      <td>0.806100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26</td>\n",
       "      <td>0.801600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27</td>\n",
       "      <td>0.785200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>28</td>\n",
       "      <td>0.851200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>29</td>\n",
       "      <td>0.778300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>0.784900</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "trainer_stats = trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "bfca4506-309c-47ac-b35c-52d3fdc50a65",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "76.2106 seconds used for training.\n",
      "1.27 minutes used for training.\n",
      "Peak reserved memory = 1.66 GB.\n",
      "Peak reserved memory for training = 0.0 GB.\n",
      "Peak reserved memory % of max memory = 11.395 %.\n",
      "Peak reserved memory for training % of max memory = 0.0 %.\n"
     ]
    }
   ],
   "source": [
    "# @title Show final memory and time stats\n",
    "used_memory = round(torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3)\n",
    "used_memory_for_lora = round(used_memory - start_gpu_memory, 3)\n",
    "used_percentage = round(used_memory / max_memory * 100, 3)\n",
    "lora_percentage = round(used_memory_for_lora / max_memory * 100, 3)\n",
    "print(f\"{trainer_stats.metrics['train_runtime']} seconds used for training.\")\n",
    "print(\n",
    "    f\"{round(trainer_stats.metrics['train_runtime']/60, 2)} minutes used for training.\"\n",
    ")\n",
    "print(f\"Peak reserved memory = {used_memory} GB.\")\n",
    "print(f\"Peak reserved memory for training = {used_memory_for_lora} GB.\")\n",
    "print(f\"Peak reserved memory % of max memory = {used_percentage} %.\")\n",
    "print(f\"Peak reserved memory for training % of max memory = {lora_percentage} %.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76c7c5ed-0593-49b4-b0dd-e94a2fe77714",
   "metadata": {},
   "source": [
    "# Save It"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e7b7915c-c8ab-436e-b6f4-5bb29cca20bb",
   "metadata": {},
   "outputs": [
    {
     "ename": "SystemExit",
     "evalue": "Execution stopped here on purpose.",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[31mSystemExit\u001b[39m\u001b[31m:\u001b[39m Execution stopped here on purpose.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.12/site-packages/IPython/core/interactiveshell.py:3557: UserWarning: To exit: use 'exit', 'quit', or Ctrl-D.\n",
      "  warn(\"To exit: use 'exit', 'quit', or Ctrl-D.\", stacklevel=1)\n"
     ]
    }
   ],
   "source": [
    "raise SystemExit(\"Execution stopped here on purpose.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2ad5875e-bbe3-4f0b-89a4-78b6ecac688a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#trainer.save_model(\"gemma3-address-parser\")\n",
    "model.save_pretrained(\"gemma3-address-parser-lora\", save_adapter=True)\n",
    "tokenizer.save_pretrained(\"gemma3-address-parser-lora\")\n",
    "\n",
    "model.config.save_pretrained(\"gemma3-address-parser-base\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "bccd8b4f-aca0-44c4-80f4-820dfdfd48c0",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==((====))==  Unsloth 2025.3.19: Fast Gemma3 patching. Transformers: 4.50.0.dev0. vLLM: 0.8.3.\n",
      "   \\\\   /|    Tesla T4. Num GPUs = 1. Max memory: 14.568 GB. Platform: Linux.\n",
      "O^O/ \\_/ \\    Torch: 2.6.0+cu124. CUDA: 7.5. CUDA Toolkit: 12.4. Triton: 3.2.0\n",
      "\\        /    Bfloat16 = FALSE. FA [Xformers = 0.0.29.post2. FA2 = False]\n",
      " \"-____-\"     Free license: http://github.com/unslothai/unsloth\n",
      "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n",
      "Unsloth: Using float16 precision for gemma3 won't work! Using float32.\n",
      "Parsing: Leclerc 10 bis route Victor Hugo 92200 Neuilly-sur-Seine \n",
      "Champs: {\"Building_Number\": 10, \"City\": \"Neuilly-sur-Seine\", \"Country\": \"France\", \"Recipient\": \"Leclerc\", \"Street_Name\": \"Victor Hugo\", \"Zip_Code\": 92200, \"repetition\": \"bis\", \"type_voie\": \"route\"}\n"
     ]
    }
   ],
   "source": [
    "from unsloth import FastModel\n",
    "model, tokenizer = FastModel.from_pretrained(\n",
    "    model_name = \"gemma3-address-parser-lora\", # YOUR MODEL YOU USED FOR TRAINING\n",
    "    max_seq_length = 2048,\n",
    "    load_in_4bit = True,\n",
    ")\n",
    "\n",
    "address = \"Leclerc 10 bis route Victor Hugo 92200 Neuilly-sur-Seine\"\n",
    "\n",
    "prompt = f\"Parsing: {address} \\nChamps:\"\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\").to(\"cuda\")\n",
    "\n",
    "outputs = model.generate(\n",
    "    **inputs,\n",
    "    max_new_tokens=100,\n",
    "    temperature = 1.0, top_p = 0.95, top_k = 64,\n",
    "    do_sample=False,\n",
    "    eos_token_id=tokenizer.eos_token_id,\n",
    "    pad_token_id=tokenizer.pad_token_id\n",
    ")\n",
    "\n",
    "print(tokenizer.decode(outputs[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "5854f63e-3694-441b-b3f9-580bca2b0c23",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'name'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAttributeError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[22]\u001b[39m\u001b[32m, line 4\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# Change to True to save to GGUF\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m4\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43msave_pretrained_merged\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mgemma3-address-parser-finetune\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      5\u001b[39m model.save_pretrained_gguf(\u001b[33m\"\u001b[39m\u001b[33mgemma3-address-parser-finetune-gguf\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m      6\u001b[39m     quantization_type = \u001b[33m\"\u001b[39m\u001b[33mF16\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;66;03m# For now only Q8_0, BF16, F16 supported\u001b[39;00m\n\u001b[32m      7\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/conda/lib/python3.12/site-packages/unsloth/save.py:2357\u001b[39m, in \u001b[36munsloth_generic_save_pretrained_merged\u001b[39m\u001b[34m(self, save_directory, tokenizer, save_method, push_to_hub, token, is_main_process, state_dict, save_function, max_shard_size, safe_serialization, variant, save_peft_format, tags, temporary_location, maximum_memory_usage)\u001b[39m\n\u001b[32m   2355\u001b[39m arguments[\u001b[33m\"\u001b[39m\u001b[33mmodel\u001b[39m\u001b[33m\"\u001b[39m] = \u001b[38;5;28mself\u001b[39m\n\u001b[32m   2356\u001b[39m \u001b[38;5;28;01mdel\u001b[39;00m arguments[\u001b[33m\"\u001b[39m\u001b[33mself\u001b[39m\u001b[33m\"\u001b[39m]\n\u001b[32m-> \u001b[39m\u001b[32m2357\u001b[39m \u001b[43munsloth_generic_save\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43marguments\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2358\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[32m3\u001b[39m):\n\u001b[32m   2359\u001b[39m     gc.collect()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/conda/lib/python3.12/site-packages/torch/utils/_contextlib.py:116\u001b[39m, in \u001b[36mcontext_decorator.<locals>.decorate_context\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    113\u001b[39m \u001b[38;5;129m@functools\u001b[39m.wraps(func)\n\u001b[32m    114\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mdecorate_context\u001b[39m(*args, **kwargs):\n\u001b[32m    115\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[32m--> \u001b[39m\u001b[32m116\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/conda/lib/python3.12/site-packages/unsloth/save.py:2304\u001b[39m, in \u001b[36munsloth_generic_save\u001b[39m\u001b[34m(model, tokenizer, save_directory, save_method, push_to_hub, token, is_main_process, state_dict, save_function, max_shard_size, safe_serialization, variant, save_peft_format, use_temp_dir, commit_message, private, create_pr, revision, commit_description, tags, temporary_location, maximum_memory_usage)\u001b[39m\n\u001b[32m   2274\u001b[39m \u001b[38;5;129m@torch\u001b[39m.inference_mode\n\u001b[32m   2275\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34munsloth_generic_save\u001b[39m(\n\u001b[32m   2276\u001b[39m     model,\n\u001b[32m   (...)\u001b[39m\u001b[32m   2301\u001b[39m     maximum_memory_usage : \u001b[38;5;28mfloat\u001b[39m = \u001b[32m0.9\u001b[39m,\n\u001b[32m   2302\u001b[39m ):\n\u001b[32m   2303\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m token \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m push_to_hub: token = get_token()\n\u001b[32m-> \u001b[39m\u001b[32m2304\u001b[39m     \u001b[43mmerge_and_overwrite_lora\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2305\u001b[39m \u001b[43m        \u001b[49m\u001b[43mget_model_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2306\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m                \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2307\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[43m            \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2308\u001b[39m \u001b[43m        \u001b[49m\u001b[43msave_directory\u001b[49m\u001b[43m       \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43msave_directory\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2309\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpush_to_hub\u001b[49m\u001b[43m          \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mpush_to_hub\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2310\u001b[39m \u001b[43m        \u001b[49m\u001b[43mprivate\u001b[49m\u001b[43m              \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mprivate\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2311\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m                \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2312\u001b[39m \u001b[43m        \u001b[49m\u001b[43moutput_dtype\u001b[49m\u001b[43m         \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m   2313\u001b[39m \u001b[43m        \u001b[49m\u001b[43mlow_disk_space_usage\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m   2314\u001b[39m \u001b[43m        \u001b[49m\u001b[43muse_temp_file\u001b[49m\u001b[43m        \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m   2315\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2316\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/conda/lib/python3.12/site-packages/torch/utils/_contextlib.py:116\u001b[39m, in \u001b[36mcontext_decorator.<locals>.decorate_context\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    113\u001b[39m \u001b[38;5;129m@functools\u001b[39m.wraps(func)\n\u001b[32m    114\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mdecorate_context\u001b[39m(*args, **kwargs):\n\u001b[32m    115\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[32m--> \u001b[39m\u001b[32m116\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/conda/lib/python3.12/site-packages/unsloth_zoo/saving_utils.py:568\u001b[39m, in \u001b[36mmerge_and_overwrite_lora\u001b[39m\u001b[34m(get_model_name, model, tokenizer, save_directory, push_to_hub, private, token, output_dtype, low_disk_space_usage, use_temp_file, cleanup_temp_file)\u001b[39m\n\u001b[32m    559\u001b[39m \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[32m    560\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m(max_size_in_bytes != \u001b[32m0\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m total_size_in_bytes != \u001b[32m0\u001b[39m)\n\u001b[32m    562\u001b[39m (\n\u001b[32m    563\u001b[39m     username, repo_id, hf_api, token,\n\u001b[32m    564\u001b[39m     output_dtype, element_size,\n\u001b[32m    565\u001b[39m     lora_weights, state_dict, save_size, free,\n\u001b[32m    566\u001b[39m     temp_file, save_directory, new_use_temp_file,\n\u001b[32m    567\u001b[39m     low_disk_space_usage, max_shard_size_in_bytes,\n\u001b[32m--> \u001b[39m\u001b[32m568\u001b[39m ) = \u001b[43mprepare_saving\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    569\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    570\u001b[39m \u001b[43m    \u001b[49m\u001b[43msave_directory\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43msave_directory\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    571\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpush_to_hub\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mpush_to_hub\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    572\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmax_shard_size\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m5GB\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    573\u001b[39m \u001b[43m    \u001b[49m\u001b[43mprivate\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mprivate\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    574\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    575\u001b[39m \u001b[43m    \u001b[49m\u001b[43moutput_dtype\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43moutput_dtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    576\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlow_disk_space_usage\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mlow_disk_space_usage\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    577\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmerge_into_original\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    578\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmin_size_in_bytes\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_size_in_bytes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    579\u001b[39m \u001b[43m    \u001b[49m\u001b[43muse_temp_file\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43muse_temp_file\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    580\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    581\u001b[39m use_temp_file = use_temp_file \u001b[38;5;129;01mor\u001b[39;00m new_use_temp_file\n\u001b[32m    583\u001b[39m n_saved_modules = \u001b[32m0\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/conda/lib/python3.12/site-packages/unsloth_zoo/saving_utils.py:472\u001b[39m, in \u001b[36mprepare_saving\u001b[39m\u001b[34m(model, save_directory, push_to_hub, max_shard_size, private, token, output_dtype, merge_into_original, low_disk_space_usage, min_size_in_bytes, use_temp_file)\u001b[39m\n\u001b[32m    470\u001b[39m \u001b[38;5;66;03m# Too small - try using the temporary file system (sometimes large like Kaggle)\u001b[39;00m\n\u001b[32m    471\u001b[39m try_temp_file = tempfile.TemporaryDirectory(ignore_cleanup_errors = \u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m--> \u001b[39m\u001b[32m472\u001b[39m try_save_directory = \u001b[43mtemp_file\u001b[49m\u001b[43m.\u001b[49m\u001b[43mname\u001b[49m\n\u001b[32m    474\u001b[39m total, used, free = shutil.disk_usage(save_directory)\n\u001b[32m    475\u001b[39m free = \u001b[38;5;28mint\u001b[39m(free*\u001b[32m0.95\u001b[39m)\n",
      "\u001b[31mAttributeError\u001b[39m: 'NoneType' object has no attribute 'name'"
     ]
    }
   ],
   "source": [
    "# Change to True to save to GGUF\n",
    "\n",
    "\n",
    "model.save_pretrained_merged(\"gemma3-address-parser-finetune\", tokenizer)\n",
    "model.save_pretrained_gguf(\"gemma3-address-parser-finetune-gguf\",\n",
    "    quantization_type = \"F16\", # For now only Q8_0, BF16, F16 supported\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "f54af5ff-a395-48af-92f3-17102a63cfe4",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/tmp/tmpjnkp__qh\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec79bc44-a36f-4d4a-a4dc-98412915bbf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from unsloth import FastLanguageModel\n",
    "\n",
    "# Load the model with LoRA adapter\n",
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name = \"gemma3-address-parser-lora\",  # The LoRA repo (with adapter_config.json)\n",
    "    base_model_name = \"unsloth/gemma-3-1b-pt-unsloth-bnb-4bit\", # The base model name (e.g. \"unsloth/gemma-2b\")\n",
    "    load_in_4bit = False,\n",
    ")\n",
    "\n",
    "# Merge LoRA into base model weights\n",
    "model = FastLanguageModel.merge_lora(model)\n",
    "\n",
    "# Save the merged model (base + LoRA)\n",
    "model.save_pretrained(\"gemma3-address-parser-merged\")\n",
    "tokenizer.save_pretrained(\"gemma3-address-parser-merged\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07c00f65-f010-4df7-8ad6-fa9a7aaf5606",
   "metadata": {},
   "source": [
    "# run it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3ab4059-ecca-4c80-83ed-03afd1851ab1",
   "metadata": {},
   "outputs": [],
   "source": [
    "address = \"Leclerc 10 bis route Victor Hugo 92200 Neuilly-sur-Seine\"\n",
    "\n",
    "prompt = f\"Parsing: {address} \\nChamps:\"\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\").to(\"cuda\")\n",
    "\n",
    "outputs = model.generate(\n",
    "    **inputs,\n",
    "    max_new_tokens=100,\n",
    "    temperature = 1.0, top_p = 0.95, top_k = 64,\n",
    "    do_sample=False,\n",
    "    eos_token_id=tokenizer.eos_token_id,\n",
    "    pad_token_id=tokenizer.pad_token_id\n",
    ")\n",
    "\n",
    "print(tokenizer.decode(outputs[0], skip_special_tokens=True))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a685ddf-afdf-4fe8-b56f-b36463857aa1",
   "metadata": {},
   "source": [
    "# use unsloth to run model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cbb5901-fb6e-42c6-98e1-02fe81d76870",
   "metadata": {},
   "outputs": [],
   "source": [
    "raise SystemExit(\"Execution stopped here on purpose.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0bab0d6-a0a0-45be-b194-1a621a1659cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# skip\n",
    "from unsloth import FastLanguageModel\n",
    "import torch\n",
    "\n",
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name = \"gemma3-address-parser-lora\", # YOUR MODEL YOU USED FOR TRAINING\n",
    "    max_seq_length = 1024,\n",
    "    dtype = torch.float16,\n",
    "    load_in_4bit = False,\n",
    ")\n",
    "FastLanguageModel.for_inference(model) # Enable native 2x faster inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3b70337-7432-4266-94fc-7c2446460d16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# skip\n",
    "address = \"Leclerc 10 bis avenue Victor Hugo 92200 Neuilly-sur-Seine\"\n",
    "prompt = f\"Parsing: {address} \\nChamps:\"\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\").to(\"cuda\")\n",
    "outputs = model.generate(\n",
    "    **inputs,\n",
    "    max_new_tokens=100,\n",
    "    temperature = 1.0, top_p = 0.95, top_k = 64,\n",
    "    do_sample=False,\n",
    "    eos_token_id=tokenizer.eos_token_id,\n",
    "    pad_token_id=tokenizer.pad_token_id\n",
    ")\n",
    "\n",
    "print(tokenizer.decode(outputs[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e766be9a-2b2a-48b4-98d8-6391597f331f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
